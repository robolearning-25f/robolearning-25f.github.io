<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>HDMI-DP: Humanoid Loco-Manipulation with Direct Perception</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <link rel="stylesheet" href="styles.css">
</head>
<body>
  <div class="container">
    <header>
      <div class="content-width">
        <h1>HDMI-DP</h1>
        <h2>Humanoid Loco-Manipulation with Direct Perception</h2>

        <div class="authors-container">
          <div class="authors-row">
            <div class="author-block"><a href="mailto:yutianch@andrew.cmu.edu">Yutian Chen</a></div>
            <div class="author-block"><a href="mailto:henrytsu@andrew.cmu.edu">Hao-Tang Tsui</a></div>
            <div class="author-block"><a href="mailto:wx2@andrew.cmu.edu">Hang Xu</a></div>
            <div class="author-block"><a href="mailto:pangchil@andrew.cmu.edu">Pang-Chi Lo</a></div>
            <div class="author-block"><a href="mailto:yurout@andrew.cmu.edu">Yu-Rou Tuan</a></div>
            <div class="author-block"><a href="mailto:jitingc@andrew.cmu.edu">Jiting Cai</a></div>
          </div>
        </div>

        <div class="affiliations">
          Carnegie Mellon University
        </div>
      </div>
    </header>

    <section>
      <div class="content-width">
        <h2>Overview</h2>
        <div class="abstract-container">
          <p>
            <strong>HDMI-DP</strong> investigates humanoid loco-manipulation under a <em>direct perception</em> paradigm, where task-relevant states—such as object contact points and relative motion—are inferred directly from onboard visual observations, without reliance on motion-capture systems or privileged simulator states.
          </p>
          <p>
            By integrating perception-driven contact estimation with humanoid control, HDMI-DP enables robust manipulation in visually rich and unstructured environments.
          </p>
          <p>
            Our approach demonstrates that perception-based pipelines can effectively replace traditional MoCap-dependent setups, while maintaining competitive performance on standard humanoid loco-manipulation tasks.
          </p>
        </div>
      </div>
    </section>

    <section>
      <div class="content-width">
        <h2>Key Idea</h2>
        <div class="teaser-centered">
          <video src="moti.webm" class="teaser" autoplay loop muted playsinline id="mainVideo"></video>
          <p style="text-align: center; margin-top: var(--space-md); font-weight: 500;">
            <strong>Direct perception enables humanoid loco-manipulation without motion capture.</strong><br>
            Instead of relying on ground-truth object poses or contact states, HDMI-DP estimates task-critical signals directly from egocentric camera observations and learned perception modules, enabling deployment in environments where external sensing infrastructure is unavailable.
          </p>
        </div>
      </div>
    </section>

    <section>
      <div class="content-width">
        <h2>Method</h2>
        <h3>Direct Perception Pipeline</h3>
        <p>
          Given egocentric RGB observations, the perception module predicts:
        </p>
        <ul style="margin-left: var(--space-xl); margin-bottom: var(--space-md);">
          <li>2D contact point locations in image space,</li>
          <li>Corresponding 3D contact positions via depth and camera geometry,</li>
          <li>Relative object motion for downstream control and state estimation.</li>
        </ul>
        <p>
          These signals are consumed directly by the humanoid controller, forming a closed-loop perception–action pipeline.
        </p>
      </div>
    </section>

    <section>
      <div class="content-width">
        <h2>Experiments</h2>

        <h3>Quantitative Evaluation</h3>
        <p style="text-align: center;">
          We evaluate perception accuracy under different camera fields of view (FOVs), measuring both image-space and 3D contact errors.
        </p>

        <table>
          <thead>
            <tr>
              <th rowspan="2">FOV</th>
              <th colspan="2">Perception error</th>
              <th colspan="2">Odometry error</th>
            </tr>
            <tr>
              <th>3D distance [cm] ↓</th>
              <th>2D error [px] ↓</th>
              <th>t<sub>rel</sub> [cm/frame]</th>
              <th>r<sub>rel</sub> [deg/frame]</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>69.4°</td>
              <td>0.384 ± 0.175</td>
              <td>0.272 ± 0.171</td>
              <td>2.210 ± 5.330</td>
              <td>1.873 ± 2.537</td>
            </tr>
            <tr>
              <td>90°</td>
              <td>0.626 ± 0.289</td>
              <td>0.278 ± 0.197</td>
              <td>1.402 ± 2.021</td>
              <td>1.506 ± 2.108</td>
            </tr>
          </tbody>
        </table>
        <p style="text-align: center; font-size: var(--text-sm); color: var(--text-light); margin-top: calc(-1 * var(--space-md)); margin-bottom: var(--space-xl);">
          Perception accuracy (3D contact points and 2D track points) and odometry drift for different egocentric camera FOVs.
        </p>

        <h3>Perception-Driven Control in Simulation</h3>
        <p style="text-align: center;">
          We evaluate HDMI-DP on standard humanoid manipulation tasks using only perception-based inputs.
        </p>

        <div class="task-grid">
          <div class="task-row">
            <div class="task-label">Push Box</div>
            <div>
              <img src="pushboxego.gif" alt="Push Box Egocentric View" class="task-media">
              <p style="text-align: center; font-size: var(--text-sm); margin-top: 8px; color: var(--text-light);">Egocentric View</p>
            </div>
            <div>
              <video id="push-box-3d-video" src="pushbox3d.webm" class="task-media" autoplay muted playsinline loop></video>
              <p style="text-align: center; font-size: var(--text-sm); margin-top: 8px; color: var(--text-light);">3D Visualization</p>
            </div>
          </div>
        </div>
        <script>
          (function () {
            const video = document.getElementById('push-box-3d-video');
            if (!video) return;
            const startTime = 5;
            const endTime = 20;

            video.addEventListener('loadedmetadata', () => {
              video.playbackRate = 2;
              video.currentTime = startTime;
            });

            video.addEventListener('timeupdate', () => {
              if (video.currentTime >= endTime) {
                video.currentTime = startTime;
              }
            });
          })();
        </script>
        <p style="text-align: center; font-size: var(--text-sm); color: var(--text-light);">
          Multi-view push-box rollouts highlight perception-only policy execution (left) and the corresponding 3D tracking visualization (right).
        </p>

        <h3>Track-Point Error Profiles</h3>
        <p style="text-align: center;">
          Track-point accuracy is characterized in both pixel space and 3D camera coordinates:
        </p>

        <div class="error-images">
          <div class="error-image-container">
            <img src="src/point_error_2d.png" alt="2D Track-Point Error" onerror="this.style.display='none'">
            <p class="error-caption">2D track-point error across the push-box sequence (pixels).</p>
          </div>
          <div class="error-image-container">
            <img src="src/point_error_3d.png" alt="3D Track-Point Error" onerror="this.style.display='none'">
            <p class="error-caption">3D track-point reconstruction error after depth lifting (centimeters).</p>
          </div>
        </div>
        <p style="text-align: center; font-style: italic; color: var(--text-light);">
          Track-point estimation remains stable over long-horizon interactions, supporting closed-loop perception-driven control.
        </p>

        <h3>Odometry Benchmarks</h3>
        <p style="text-align: center;">
          Stereo MAC-VO runs entirely onboard and replaces motion-capture root pose with camera-only odometry; below are qualitative rollouts and quantitative drift metrics.
        </p>

        <div class="media-grid">
          <figure>
            <video src="flow_odometry.mp4" autoplay muted loop playsinline controls></video>
            <figcaption class="media-caption">Flow odometry with dense tracked trails.</figcaption>
          </figure>
          <figure>
            <video src="twoview_odometry.mp4" autoplay muted loop playsinline controls></video>
            <figcaption class="media-caption">Two-view odometry baseline enforcing geometric checks.</figcaption>
          </figure>
        </div>

        <div class="media-grid">
          <figure>
            <img src="src/Combined_RTEcdf.png" alt="Translational Error CDF">
            <figcaption class="media-caption">Translational error CDF from MAC-VO stereo odometry.</figcaption>
          </figure>
          <figure>
            <img src="src/Combined_ROEcdf.png" alt="Rotational Error CDF">
            <figcaption class="media-caption">Rotational error CDF highlighting bounded heading drift.</figcaption>
          </figure>
        </div>

        <div class="media-grid">
          <figure>
            <img src="src/MACVO_hdmi_narrow_fov_TranslationErr.png" alt="MACVO Translation Error">
            <figcaption class="media-caption">Translational drift over time under a narrow FOV.</figcaption>
          </figure>
          <figure>
            <img src="src/MACVO_hdmi_narrow_fov_RotationErr.png" alt="MACVO Rotation Error">
            <figcaption class="media-caption">Rotational drift profile showing low orientation error.</figcaption>
          </figure>
        </div>

        <p style="text-align: center; font-style: italic; color: var(--text-light);">
          Flow cues and multi-view geometry provide complementary odometry signals that remain bounded even under narrow FOVs.
        </p>
      </div>
    </section>

    <section>
      <div class="content-width">
        <h2>Summary</h2>
        <div class="summary-list">
          <p style="font-weight: 600; margin-bottom: var(--space-md); color: var(--primary);">
            HDMI-DP demonstrates that:
          </p>
          <ul>
            <li>Humanoid loco-manipulation can be achieved using <em>direct visual perception</em> alone,</li>
            <li>Motion-capture systems and privileged simulator states are not strictly necessary,</li>
            <li>Perception-driven pipelines generalize across manipulation tasks and camera configurations.</li>
          </ul>
          <p style="margin-top: var(--space-md); font-style: italic;">
            This work supports a shift toward perception-centric humanoid systems that operate robustly in realistic, uninstrumented environments.
          </p>
        </div>
      </div>
    </section>

    <footer>
      <div class="content-width">
        <p>&copy; 2025 HDMI-DP Team | Carnegie Mellon University</p>
      </div>
    </footer>
  </div>

  <script>
    // Intersection Observer for section animations
    document.addEventListener('DOMContentLoaded', function() {
      const sections = document.querySelectorAll('section');
      const observer = new IntersectionObserver((entries) => {
        entries.forEach(entry => {
          if (entry.isIntersecting) {
            entry.target.classList.add('visible');
          }
        });
      }, { threshold: 0.1 });

      sections.forEach(section => {
        observer.observe(section);
      });

      // Set video playback speed to 10x and start at 30 seconds
      const mainVideo = document.getElementById('mainVideo');
      if (mainVideo) {
        mainVideo.playbackRate = 10.0;
        mainVideo.currentTime = 30;
      }
    });
  </script>
</body>
</html>
